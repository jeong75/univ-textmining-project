{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": []
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "MWStYrI52Vwr"
      },
      "outputs": [],
      "source": [
        "from google.colab import drive\n",
        "drive.mount('/content/drive')"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "import pandas as pd\n",
        "import glob\n",
        "import os"
      ],
      "metadata": {
        "id": "JWK7NLgX2XUu"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "!pip install kiwipiepy"
      ],
      "metadata": {
        "id": "nVDJJ1032Yd9"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "클렌징, 토큰화, 불용어 제거, 어간 추출 + 기본형(감정분석용)"
      ],
      "metadata": {
        "id": "T0ypgoC12beb"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "import pandas as pd\n",
        "import glob\n",
        "import os\n",
        "from kiwipiepy import Kiwi\n",
        "import re\n",
        "from tqdm import tqdm\n",
        "from kiwipiepy import Kiwi\n",
        "kiwi = Kiwi()\n",
        "\n",
        "INPUT_DIR = '/content/drive/My Drive/project/리뷰DB'\n",
        "OUTPUT_DIR = '/content/drive/My Drive/project/리뷰DB_tokenized'\n",
        "\n",
        "REVIEW_COLUMN = 'review_text'"
      ],
      "metadata": {
        "id": "4LWqrrqK2aGt"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "import re\n",
        "\n",
        "HYPHENS = r\"[\\u2010\\u2011\\u2012\\u2013\\u2014\\u2212\\-]\"\n",
        "\n",
        "PROTECT_PHRASES = [\n",
        "    (r\"자기\\s*[-]?\\s*계발\", \"자기계발\"),\n",
        "    (r\"퍼스널\\s*[-]?\\s*브랜딩\", \"퍼스널브랜딩\"),\n",
        "    (r\"리\\s*[-]?\\s*브랜딩\", \"리브랜딩\"),\n",
        "    (r\"브\\s*[-]?\\s*랜딩\", \"브랜딩\"),\n",
        "]\n",
        "\n",
        "def normalize_phrases(text: str) -> str:\n",
        "    text = str(text)\n",
        "    text = re.sub(HYPHENS, \"-\", text)\n",
        "    for pat, repl in PROTECT_PHRASES:\n",
        "        text = re.sub(pat, repl, text)\n",
        "    return text"
      ],
      "metadata": {
        "id": "BhHvbHUM2dC9"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# 토큰화 제외 대상 정의\n",
        "for w in [\"자기 계발\", \"자기계발\", \"퍼스널브랜딩\", \"리브랜딩\",\n",
        "          \"브랜딩\", \"강추\",\"비추\",\"최애\",\"흔한 남매\",\"흔한남매\"]:\n",
        "    kiwi.add_user_word(w, \"NNG\")"
      ],
      "metadata": {
        "id": "yCZDK5t02eS5"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# 불용어(Stopwords) 리스트 정의\n",
        "stopwords = {\n",
        "    '책','작가','저자','구매','주문', '배송', '도서', '리뷰','서평','발송',\n",
        "    'yes24','교보문고' '내용','포장','이야기',\n",
        "    '읽다','읽히','그것','무엇',\n",
        "    '그', '이', '저', '것', '수', '등', '들','때', '거', '해서',\n",
        "    '하지만', '그리고', '그래서',\n",
        "    '이다', '보다','보는',\n",
        "    '그냥','사실','개인적으로','솔직히',\n",
        "    '왠만해선','정말로'\n",
        "}"
      ],
      "metadata": {
        "id": "8pxDkAXh2fXM"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "def extract_quad_tokens(text):\n",
        "    text = normalize_phrases(text)\n",
        "\n",
        "    text_cleaned = re.sub(r'[^가-힣A-Za-z0-9\\s]', '', str(text))\n",
        "    if not text_cleaned.strip():\n",
        "        return [], [], [], []\n",
        "\n",
        "    try:\n",
        "        res = kiwi.tokenize(text_cleaned)\n",
        "    except:\n",
        "        return [], [], [], []\n",
        "\n",
        "    tokens_surface, tokens_lemma = [], []\n",
        "    sentiment_tokens, sentiment_lemmas = [], []\n",
        "\n",
        "    for t in res:\n",
        "        if t.tag in ['NNG', 'NNP', 'VV', 'VA', 'XR', 'MAG', 'SL']:\n",
        "            tokens_surface.append(t.form)\n",
        "            tokens_lemma.append(t.lemma)\n",
        "\n",
        "            if t.lemma in stopwords or t.form in stopwords:\n",
        "                continue\n",
        "\n",
        "            sentiment_tokens.append(t.form)\n",
        "            sentiment_lemmas.append(t.lemma)\n",
        "\n",
        "    return tokens_surface, tokens_lemma, sentiment_tokens, sentiment_lemmas"
      ],
      "metadata": {
        "id": "3XxUnMms2gso"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "def process_review_db(input_dir, output_dir, target_col):\n",
        "    print(\"[Start] 리뷰 DB 전처리 및 토큰화를 시작합니다...\\n\")\n",
        "\n",
        "    if not os.path.exists(output_dir):\n",
        "        os.makedirs(output_dir)\n",
        "    else:\n",
        "        print(f\"결과 폴더 확인: {output_dir}\")\n",
        "\n",
        "    csv_files = glob.glob(os.path.join(input_dir, '*.csv'))\n",
        "\n",
        "    if not csv_files:\n",
        "        print(f\" '{input_dir}' 경로에 CSV 파일이 없습니다.\")\n",
        "        return\n",
        "\n",
        "    print(f\"총 {len(csv_files)}개의 파일을 처리합니다.\")\n",
        "\n",
        "    for file_path in tqdm(csv_files, desc=\"Processing Files\"):\n",
        "        filename = os.path.basename(file_path)\n",
        "        save_path = os.path.join(output_dir, filename)\n",
        "\n",
        "        try:\n",
        "            df = pd.read_csv(file_path)\n",
        "\n",
        "            if target_col in df.columns:\n",
        "                df[target_col] = df[target_col].fillna('')\n",
        "\n",
        "                results = df[target_col].apply(extract_quad_tokens)\n",
        "\n",
        "                df['tokens'] = [r[0] for r in results]\n",
        "                df['tokens_lemma'] = [r[1] for r in results]\n",
        "                df['sentiment_tokens'] = [r[2] for r in results]\n",
        "                df['sentiment_lemmas'] = [r[3] for r in results]\n",
        "\n",
        "                df.to_csv(save_path, index=False, encoding='utf-8-sig')\n",
        "\n",
        "            else:\n",
        "                df.to_csv(save_path, index=False, encoding='utf-8-sig')\n",
        "\n",
        "        except Exception as e:\n",
        "            print(f\"Error processing {filename}: {e}\")\n",
        "\n",
        "    print(\"\\n모든 작업이 성공적으로 끝났습니다!\")\n",
        "\n",
        "# 실행\n",
        "if __name__ == \"__main__\":\n",
        "    process_review_db(INPUT_DIR, OUTPUT_DIR, REVIEW_COLUMN)"
      ],
      "metadata": {
        "id": "U-Y5MsrQ2h5a"
      },
      "execution_count": null,
      "outputs": []
    }
  ]
}