{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": []
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "code",
      "source": [
        "# ÌïúÍ∏Ä Ìè∞Ìä∏ ÏÑ§Ïπò\n",
        "!sudo apt-get install -y fonts-nanum\n",
        "!sudo fc-cache -fv\n",
        "!rm ~/.cache/matplotlib -rf"
      ],
      "metadata": {
        "id": "0an2xVCEFafe"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "import pandas as pd\n",
        "import matplotlib.pyplot as plt\n",
        "import seaborn as sns\n",
        "from google.colab import drive\n",
        "import os\n",
        "\n",
        "# Íµ¨Í∏Ä ÎìúÎùºÏù¥Î∏å Ïó∞Í≤∞\n",
        "drive.mount('/content/drive')"
      ],
      "metadata": {
        "id": "OiJAw8ZbFb5Z"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "import os, glob, ast, json\n",
        "import pandas as pd\n",
        "from tqdm import tqdm\n",
        "from collections import defaultdict\n",
        "\n",
        "TOKENIZED_DIR = \"/content/drive/MyDrive/project/Î¶¨Î∑∞DB_tokenized\"\n",
        "OUT_DIR       = \"/content/drive/MyDrive/project/Î¶¨Î∑∞DB_scored\"\n",
        "os.makedirs(OUT_DIR, exist_ok=True)\n",
        "\n",
        "SENT_TOKEN_COL = \"sentiment_tokens\"\n",
        "LEXICON_JSON_PATH = \"/content/drive/MyDrive/project/SentiWord_info.json\" # KNU\n",
        "\n",
        "# Î∂ÄÏ†ï/Í∞ïÏ°∞\n",
        "USE_RULES = True\n",
        "NEGATORS = {\"Ïïà\", \"Î™ª\"}\n",
        "DOWNERS  = {\"Î≥ÑÎ°ú\"}\n",
        "BOOSTERS = {\"ÎÑàÎ¨¥\", \"ÏßÑÏßú\", \"ÏôÑÏ†Ñ\"}\n",
        "\n",
        "def to_list(x):\n",
        "    if isinstance(x, list):\n",
        "        return x\n",
        "    if pd.isna(x):\n",
        "        return []\n",
        "    if isinstance(x, str):\n",
        "        try:\n",
        "            v = ast.literal_eval(x)\n",
        "            return v if isinstance(v, list) else []\n",
        "        except:\n",
        "            return [t.strip() for t in x.split(\",\") if t.strip()]\n",
        "    return []\n",
        "\n",
        "# Í∞êÏÑ±ÏÇ¨Ï†Ñ Î°úÎìú: Îã®Ïñ¥(1Í∑∏Îû®) / ÌëúÌòÑ(2~NÍ∑∏Îû®) Î∂ÑÎ¶¨ (word_root Í∏∞Ï§Ä)\n",
        "with open(LEXICON_JSON_PATH, \"r\", encoding=\"utf-8\") as f:\n",
        "    lex_data = json.load(f)\n",
        "\n",
        "single_lex = {}\n",
        "phrase_lex_by_len = defaultdict(dict)\n",
        "\n",
        "for item in lex_data:\n",
        "    root = str(item.get(\"word_root\", \"\")).strip()\n",
        "    if not root:\n",
        "        continue\n",
        "\n",
        "    pol = int(str(item.get(\"polarity\", \"0\")).strip())\n",
        "    toks = tuple(root.split())\n",
        "\n",
        "    if len(toks) == 1:\n",
        "        single_lex[toks[0]] = pol\n",
        "    else:\n",
        "        phrase_lex_by_len[len(toks)][toks] = pol\n",
        "\n",
        "max_phrase_len = max(phrase_lex_by_len.keys(), default=1)\n",
        "\n",
        "# ngramÏö∞ÏÑ† Îß§Ïπ≠ + Î∂ÄÏ†ï/Í∞ïÏ°∞ Î£∞\n",
        "def find_phrase(tokens, start_idx):\n",
        "    n = len(tokens)\n",
        "    maxL = min(max_phrase_len, n - start_idx)\n",
        "    for L in range(maxL, 1, -1):\n",
        "        key = tuple(tokens[start_idx:start_idx+L])\n",
        "        d = phrase_lex_by_len.get(L)\n",
        "        if d and key in d:\n",
        "            return L, d[key], \" \".join(key)\n",
        "    return 0, 0, \"\"\n",
        "\n",
        "def score_tokens_ngram(tokens):\n",
        "    \"\"\"\n",
        "    tokens: sentiment_tokens(form) Î¶¨Ïä§Ìä∏\n",
        "    return: total_score, matched_cnt, pos_cnt, neg_cnt, matched_terms\n",
        "    \"\"\"\n",
        "    total = 0\n",
        "    matched_cnt = 0\n",
        "    pos_cnt = 0\n",
        "    neg_cnt = 0\n",
        "    matched_terms = []\n",
        "\n",
        "    i = 0\n",
        "    n = len(tokens)\n",
        "\n",
        "    while i < n:\n",
        "        w = tokens[i]\n",
        "\n",
        "        # ---- (A) Î∂ÄÏ†ï/Í∞ïÏ°∞ Îã®Ïñ¥Í∞Ä ÏûàÏúºÎ©¥, Îã§Ïùå Íµ¨Í∞ÑÏùÑ 'ÌëúÌòÑ Ïö∞ÏÑ†'ÏúºÎ°ú Ï∞æÏïÑ Ï†ÅÏö© ----\n",
        "        if USE_RULES and w in (NEGATORS | DOWNERS | BOOSTERS) and i + 1 < n:\n",
        "            L, s, txt = find_phrase(tokens, i + 1)\n",
        "            if L == 0:\n",
        "                nxt = tokens[i + 1]\n",
        "                s = single_lex.get(nxt, 0)\n",
        "                txt = nxt\n",
        "                L = 1\n",
        "\n",
        "            if s != 0:\n",
        "                if w in NEGATORS:\n",
        "                    s2 = int(round(-0.8 * s))  # Î∞òÏ†Ñ+Í∞êÏá†\n",
        "                    total += s2\n",
        "                    matched_cnt += 1\n",
        "                    pos_cnt += 1 if s2 > 0 else 0\n",
        "                    neg_cnt += 1 if s2 < 0 else 0\n",
        "                    matched_terms.append((f\"{w} {txt}\", s2))\n",
        "                    i += 1 + L\n",
        "                    continue\n",
        "\n",
        "                if w in DOWNERS:\n",
        "                    s2 = -max(1, abs(s))       # 'Î≥ÑÎ°ú'Îäî Î∂ÄÏ†ï Í∞ïÌôî\n",
        "                    total += s2\n",
        "                    matched_cnt += 1\n",
        "                    neg_cnt += 1\n",
        "                    matched_terms.append((f\"{w} {txt}\", s2))\n",
        "                    i += 1 + L\n",
        "                    continue\n",
        "\n",
        "                if w in BOOSTERS:\n",
        "                    s2 = 2 if s > 0 else -2    # Í∞ïÎèÑ Í∞ïÌôî(Îã®Ïàú Î≤ÑÏ†Ñ)\n",
        "                    total += s2\n",
        "                    matched_cnt += 1\n",
        "                    pos_cnt += 1 if s2 > 0 else 0\n",
        "                    neg_cnt += 1 if s2 < 0 else 0\n",
        "                    matched_terms.append((f\"{w} {txt}\", s2))\n",
        "                    i += 1 + L\n",
        "                    continue\n",
        "\n",
        "        # ---- (B) ÏùºÎ∞ò: ÌëúÌòÑ(ngram) Ïö∞ÏÑ† Îß§Ïπ≠ ----\n",
        "        L, s, txt = find_phrase(tokens, i)\n",
        "        if L > 0:\n",
        "            total += s\n",
        "            matched_cnt += 1\n",
        "            pos_cnt += 1 if s > 0 else 0\n",
        "            neg_cnt += 1 if s < 0 else 0\n",
        "            matched_terms.append((txt, s))\n",
        "            i += L\n",
        "            continue\n",
        "\n",
        "        # ---- (C) Îã®Ïñ¥(1Í∑∏Îû®) Îß§Ïπ≠ ----\n",
        "        s = single_lex.get(w, 0)\n",
        "        if s != 0:\n",
        "            total += s\n",
        "            matched_cnt += 1\n",
        "            pos_cnt += 1 if s > 0 else 0\n",
        "            neg_cnt += 1 if s < 0 else 0\n",
        "            matched_terms.append((w, s))\n",
        "\n",
        "        i += 1\n",
        "\n",
        "    return total, matched_cnt, pos_cnt, neg_cnt, matched_terms\n",
        "\n",
        "files = sorted(glob.glob(os.path.join(TOKENIZED_DIR, \"*.csv\")))\n",
        "print(\"üìÑ Ï≤òÎ¶¨Ìï† ÌååÏùº Ïàò:\", len(files))\n",
        "\n",
        "summary_rows = []\n",
        "\n",
        "for path in tqdm(files, desc=\"Scoring CSVs\"):\n",
        "    # ÏùΩÍ∏∞\n",
        "    try:\n",
        "        df = pd.read_csv(path)\n",
        "    except Exception:\n",
        "        df = pd.read_csv(path, encoding=\"cp949\")\n",
        "\n",
        "    if SENT_TOKEN_COL not in df.columns:\n",
        "        out_path = os.path.join(OUT_DIR, os.path.basename(path))\n",
        "        df.to_csv(out_path, index=False, encoding=\"utf-8-sig\")\n",
        "        continue\n",
        "\n",
        "    # Î¶¨Ïä§Ìä∏ Î≥µÏõê\n",
        "    df[SENT_TOKEN_COL] = df[SENT_TOKEN_COL].apply(to_list)\n",
        "\n",
        "    # Ïä§ÏΩîÏñ¥ÎßÅ\n",
        "    scored = df[SENT_TOKEN_COL].apply(score_tokens_ngram)\n",
        "    df[\"sent_score\"] = [x[0] for x in scored]\n",
        "    df[\"sent_matched_cnt\"] = [x[1] for x in scored]\n",
        "    df[\"sent_pos_cnt\"] = [x[2] for x in scored]\n",
        "    df[\"sent_neg_cnt\"] = [x[3] for x in scored]\n",
        "    df[\"sent_matched_terms\"] = [x[4] for x in scored]\n",
        "\n",
        "    # Îß§Ïπ≠Î•†\n",
        "    df[\"sent_len\"] = df[SENT_TOKEN_COL].apply(len)\n",
        "    df[\"sent_match_rate\"] = df.apply(\n",
        "        lambda r: (r[\"sent_matched_cnt\"] / r[\"sent_len\"]) if r[\"sent_len\"] > 0 else 0,\n",
        "        axis=1\n",
        "    )\n",
        "\n",
        "    # 2ÏßÑ ÎùºÎ≤®: Í∏ç=1, Î∂Ä=0\n",
        "    df[\"sent_binary\"] = (df[\"sent_score\"] > 0).astype(int)\n",
        "\n",
        "    out_path = os.path.join(OUT_DIR, os.path.basename(path))\n",
        "    df.to_csv(out_path, index=False, encoding=\"utf-8-sig\")\n",
        "\n",
        "    summary_rows.append({\n",
        "        \"file\": os.path.basename(path),\n",
        "        \"n_rows\": len(df),\n",
        "        \"avg_score\": df[\"sent_score\"].mean(),\n",
        "        \"pos_ratio(=1)\": df[\"sent_binary\"].mean(),         # 1Ïùò ÎπÑÏú®\n",
        "        \"neg_ratio(=0)\": 1 - df[\"sent_binary\"].mean(),     # 0Ïùò ÎπÑÏú®\n",
        "        \"avg_match_rate\": df[\"sent_match_rate\"].mean()\n",
        "    })\n",
        "\n",
        "summary = pd.DataFrame(summary_rows).sort_values(\"avg_score\", ascending=False)\n",
        "summary_path = os.path.join(OUT_DIR, \"_summary_by_file.csv\")\n",
        "summary.to_csv(summary_path, index=False, encoding=\"utf-8-sig\")\n",
        "\n",
        "print(\"\\n‚úÖ ÏôÑÎ£å!\")\n",
        "print(\"üìÅ Í≤∞Í≥º Ìè¥Îçî:\", OUT_DIR)\n",
        "print(\"üìÑ ÏöîÏïΩ ÌååÏùº:\", summary_path)\n",
        "display(summary.head(20))\n"
      ],
      "metadata": {
        "id": "O56tKoz_PDu2"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [],
      "metadata": {
        "id": "2AVNA1vZD8vM"
      },
      "execution_count": null,
      "outputs": []
    }
  ]
}